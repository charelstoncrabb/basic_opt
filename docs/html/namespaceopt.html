<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.14"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Basic C++ optimization library: opt Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Basic C++ optimization library
   &#160;<span id="projectnumber">0.0</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.14 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
var searchBox = new SearchBox("searchBox", "search",false,'Search');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:cf05388f2679ee054f2beb29a391d25f4e673ac3&amp;dn=gpl-2.0.txt GPL-v2 */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="summary">
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">opt Namespace Reference</div>  </div>
</div><!--header-->
<div class="contents">

<p>Optimization namespace containing various optimization algorithms.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:af1ef2e32062af31429ae74fc07c57fb0"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceopt.html#af1ef2e32062af31429ae74fc07c57fb0">gradientDescent</a> (std::function&lt; double(double)&gt;f, double &amp;x0, double tol=1.e-6, bool verbose=true)</td></tr>
<tr class="memdesc:af1ef2e32062af31429ae74fc07c57fb0"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute a local minimum near initial guess of real-valued function.  <a href="#af1ef2e32062af31429ae74fc07c57fb0">More...</a><br /></td></tr>
<tr class="separator:af1ef2e32062af31429ae74fc07c57fb0"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a7db27c86e1c5a503b7f8373ba067d97b"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceopt.html#a7db27c86e1c5a503b7f8373ba067d97b">gradientDescent_Xd</a> (std::function&lt; double(std::vector&lt; double &gt;)&gt;f, std::vector&lt; double &gt; &amp;x0, double tol=1.e-6, bool verbose=true)</td></tr>
<tr class="memdesc:a7db27c86e1c5a503b7f8373ba067d97b"><td class="mdescLeft">&#160;</td><td class="mdescRight">Compute a local minimum near initial guess of real-valued function.  <a href="#a7db27c86e1c5a503b7f8373ba067d97b">More...</a><br /></td></tr>
<tr class="separator:a7db27c86e1c5a503b7f8373ba067d97b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8f434753958049daab7129ea247a97ff"><td class="memItemLeft" align="right" valign="top">double&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="namespaceopt.html#a8f434753958049daab7129ea247a97ff">ncgd_Xd</a> (std::function&lt; double(std::vector&lt; double &gt;)&gt;f, std::vector&lt; double &gt; &amp;x0, double tol=1.e-6, bool verbose=true)</td></tr>
<tr class="memdesc:a8f434753958049daab7129ea247a97ff"><td class="mdescLeft">&#160;</td><td class="mdescRight">Nonlinear conjugate gradient descent algorithm of real-valued function.  <a href="#a8f434753958049daab7129ea247a97ff">More...</a><br /></td></tr>
<tr class="separator:a8f434753958049daab7129ea247a97ff"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>Optimization namespace containing various optimization algorithms. </p>
<p>Currently implemented are the basic Barzilai-Borwein gradient descent methods (<em><a href="https://en.wikipedia.org/wiki/Gradient_descent">see Wikipedia's 'Gradient Descent' article</a></em>) for both scalar-valued functions of a single variable <img class="formulaInl" alt="$f:R\longrightarrow R$" src="form_0.png"/> as well as scalar-valued functions of <img class="formulaInl" alt="$d$" src="form_1.png"/> variables <img class="formulaInl" alt="$ f:R^d\longrightarrow R$" src="form_2.png"/>. <br />
<br />
 The gradient descent algorithms implement the following iterative method: </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ x_{n+1}=x_n-\gamma_n\nabla f(x_n), \]" src="form_3.png"/>
</p>
<p> where </p><p class="formulaDsp">
<img class="formulaDsp" alt="\[ \gamma_n=\frac{(x_n-x_{n-1})\cdot\left[\nabla f(x_n)-\nabla f(x_{n-1})\right]}{\left|\nabla f(x_n)-\nabla f(x_{n-1})\right|^2} \]" src="form_4.png"/>
</p>
<p> and <img class="formulaInl" alt="$\nabla f$" src="form_5.png"/> is approximated with a simple finite-difference method using <img class="formulaInl" alt="$\Delta x=\min(tol/10,\gamma_n/10)$" src="form_6.png"/>, where <img class="formulaInl" alt="$tol$" src="form_7.png"/> is a provided convergence tolerance. <br />
<br />
 The nonlinear conjugate gradient method implements the algorithm outlined in the <a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method">Wikipedia article</a>, with the <img class="formulaInl" alt="$\beta_n$" src="form_15.png"/> parameter coming from the Polak-Ribiere formula. Note that this implementation performs the line search step of the algorithm using lambda expressions, so that the library has an implicit dependency on the C++11 standard. <br />
<br />
 For functions of <img class="formulaInl" alt="$d$" src="form_1.png"/> variables, a <b>std::vector&lt;double&gt;</b> container is assumed as the vector argument. <br />
<br />
 The following presents a simple example of using these optimizers. <br />
 </p><div class="fragment"><div class="line"><span class="preprocessor">#include &lt;vector&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;iostream&gt;</span></div><div class="line"><span class="preprocessor">#include &lt;stdexcept&gt;</span></div><div class="line"><span class="preprocessor">#include &quot;opt.h&quot;</span></div><div class="line"></div><div class="line"><span class="comment">// Single-variable f(x) = x^2</span></div><div class="line"><span class="keywordtype">double</span> x2(<span class="keywordtype">double</span> x)</div><div class="line">{</div><div class="line">    <span class="keywordflow">return</span> x * x;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="comment">// Multi-variate f(x) = |x|^2</span></div><div class="line"><span class="keywordtype">double</span> x2_Xd(std::vector&lt;double&gt; x)</div><div class="line">{</div><div class="line">    <span class="keywordtype">size_t</span> N = x.size();</div><div class="line">    <span class="keywordtype">double</span> ans = 0.0;</div><div class="line">    <span class="keywordflow">for</span> (<span class="keywordtype">size_t</span> i = 0; i &lt; N; i++)</div><div class="line">        ans += x[i] * x[i];</div><div class="line">    <span class="keywordflow">return</span> ans;</div><div class="line">}</div><div class="line"></div><div class="line"><span class="comment">// Rosenbrock function</span></div><div class="line"><span class="keywordtype">double</span> rosenbrock(std::vector&lt;double&gt; x)</div><div class="line">{</div><div class="line">    <span class="keywordflow">if</span> (x.size() != 2)</div><div class="line">        std::runtime_error::runtime_error(<span class="stringliteral">&quot;rosenbrock function only takes 2D vectors as input&quot;</span>);</div><div class="line"></div><div class="line">    <span class="keywordflow">return</span> (1 - x[0])*(1 - x[0]) + 100.0*(x[1] - x[0] * x[0])*(x[1] - x[0] * x[0]);</div><div class="line">}</div><div class="line"></div><div class="line"><span class="keywordtype">int</span> main()</div><div class="line">{</div><div class="line">    <span class="comment">// Single-variable problem parameters:</span></div><div class="line">    <span class="keywordtype">double</span> x0 = 8.;</div><div class="line">    <span class="keywordtype">double</span> x0_init = x0;</div><div class="line"></div><div class="line">    <span class="comment">/*** Call the single-variable gradient descent optimizer on the x^2 function: ***/</span></div><div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Solution for min val of f(x) = x^2 with initial guess x0 = &quot;</span> &lt;&lt; x0_init &lt;&lt; <span class="stringliteral">&quot;:&quot;</span> &lt;&lt; std::endl;</div><div class="line">    <span class="keywordtype">double</span> gdval = <a class="code" href="namespaceopt.html#af1ef2e32062af31429ae74fc07c57fb0">opt::gradientDescent</a>(x2, x0);</div><div class="line"></div><div class="line">    std::cout &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="comment">// Multi-variable problem parameters:</span></div><div class="line">    std::vector&lt;double&gt; X0 = { 4.,4. };</div><div class="line">    std::vector&lt;double&gt; X0_init = X0;</div><div class="line"></div><div class="line">    <span class="comment">/*** Call the multi-variable gradient descent optimizer on the |x|^2 function: ***/</span></div><div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Solution for min val of f(x) = |x|^2 with initial guess X0 = [&quot;</span> &lt;&lt; X0_init[0] &lt;&lt; <span class="stringliteral">&quot;, &quot;</span> &lt;&lt; X0_init[1] &lt;&lt; <span class="stringliteral">&quot;]:&quot;</span> &lt;&lt; std::endl;</div><div class="line">    gdval = <a class="code" href="namespaceopt.html#a7db27c86e1c5a503b7f8373ba067d97b">opt::gradientDescent_Xd</a>(x2_Xd, X0);</div><div class="line"></div><div class="line">    std::cout &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="comment">// Re-initialize parameters for rosenbrock opt problem</span></div><div class="line">    X0 = {-0.5, 0.5};</div><div class="line">    X0_init = X0;</div><div class="line"></div><div class="line">    <span class="comment">/*** Call the multi-variable gradient descent optimizer on the Rosenbrock function: ***/</span></div><div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Solution for min val of rosenbrock(x) with initial guess X0 = [&quot;</span> &lt;&lt; X0_init[0] &lt;&lt; <span class="stringliteral">&quot;, &quot;</span> &lt;&lt; X0_init[1] &lt;&lt; <span class="stringliteral">&quot;]:&quot;</span> &lt;&lt; std::endl;</div><div class="line">    gdval = <a class="code" href="namespaceopt.html#a7db27c86e1c5a503b7f8373ba067d97b">opt::gradientDescent_Xd</a>(rosenbrock, X0, 1.e-10);</div><div class="line"></div><div class="line">    std::cout &lt;&lt; std::endl;</div><div class="line"></div><div class="line">    <span class="comment">// Re-initialize parameters for rosenbrock opt problem with nonlinear conjugate gradient method</span></div><div class="line">    X0 = { -0.5, 0.5 };</div><div class="line">    X0_init = X0;</div><div class="line"></div><div class="line">    <span class="comment">/*** Call the nonlinear conjugate gradient optimizer on the Rosenbrock function: ***/</span></div><div class="line">    std::cout &lt;&lt; <span class="stringliteral">&quot;Solution for min val of rosenbrock(x) with initial guess X0 = [&quot;</span> &lt;&lt; X0_init[0] &lt;&lt; <span class="stringliteral">&quot;, &quot;</span> &lt;&lt; X0_init[1] &lt;&lt; <span class="stringliteral">&quot;]:&quot;</span> &lt;&lt; std::endl;</div><div class="line">    gdval = <a class="code" href="namespaceopt.html#a8f434753958049daab7129ea247a97ff">opt::ncgd_Xd</a>(rosenbrock, X0);</div><div class="line"></div><div class="line">    std::cin.get();</div><div class="line">    <span class="keywordflow">return</span> 0;</div><div class="line">}</div></div><!-- fragment --><p> <br />
<br />
 This program, ran using VS compiler on a Windows 10 64-bit machine, output the following: <br />
 </p><div class="fragment"><div class="line">Minimum converged in 2 iterations</div><div class="line">Solution for min val of f(x) = x^2 with initial guess x0 = 8:</div><div class="line">        Minimum = 2.5e-15, x_min = -5e-08</div><div class="line"></div><div class="line">Minimum converged in 2 iterations</div><div class="line">Solution for min val of f(x) = |x|^2 with initial guess X0 = [4, 4]:</div><div class="line">        Minimum = 5e-15, x_min = [-5e-08, -5e-08]</div><div class="line"></div><div class="line">Minimum converged in 55 iterations</div><div class="line">Solution for min val of rosenbrock(x) with initial guess X0 = [-0.5, 0.5]:</div><div class="line">    Minimum = 9.83781e-11, x_min = [1, 1]</div><div class="line"></div><div class="line">Minimum converged in 20 iterations</div><div class="line">Solution for min val of rosenbrock(x) with initial guess X0 = [-0.5, 0.5]:</div><div class="line">    Minimum = 1.97327e-12, x_min = [1, 1]</div></div><!-- fragment --> </div><h2 class="groupheader">Function Documentation</h2>
<a id="af1ef2e32062af31429ae74fc07c57fb0"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af1ef2e32062af31429ae74fc07c57fb0">&#9670;&nbsp;</a></span>gradientDescent()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double opt::gradientDescent </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; double(double)&gt;&#160;</td>
          <td class="paramname"><em>f</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double &amp;&#160;</td>
          <td class="paramname"><em>x0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>tol</em> = <code>1.e-6</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute a local minimum near initial guess of real-valued function. </p>
<p>This function implements the unconstrained Barzilai-Borwein gradient descent method (<em><a href="https://en.wikipedia.org/wiki/Gradient_descent">see Wikipedia's 'Gradient Descent' article</a></em>) for a general single variable real-valued function. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">f</td><td><b>(std::function&lt;double(double)&gt;)</b> real-valued function <img class="formulaInl" alt="$f:R\longrightarrow R$" src="form_0.png"/> to find the local minimum nearest initial guess <img class="formulaInl" alt="$x_0$" src="form_8.png"/>. <img class="formulaInl" alt="$f$" src="form_11.png"/> is a function that has <b>double</b> arg type, and <b>double</b> as return type. </td></tr>
    <tr><td class="paramname">x0</td><td><b>(double&amp;)</b> Initial guess of gradient descent optimization problem; this parameter is updated with the optimized <img class="formulaInl" alt="$x-$" src="form_9.png"/>value prior to exiting. </td></tr>
    <tr><td class="paramname">tol</td><td><b>(double)</b> by default 1.0e-6, the convergence criterion for function derivative magnitude </td></tr>
    <tr><td class="paramname">verbose</td><td><b>(bool)</b> by default true, if set to true then the optimizer will echo to stdout number of iterations performed </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><img class="formulaInl" alt="$f(x_{opt})$" src="form_13.png"/> <b>(double)</b> local minimum of <img class="formulaInl" alt="$f$" src="form_11.png"/> in well of <img class="formulaInl" alt="$x_0$" src="form_8.png"/> </dd></dl>

</div>
</div>
<a id="a7db27c86e1c5a503b7f8373ba067d97b"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a7db27c86e1c5a503b7f8373ba067d97b">&#9670;&nbsp;</a></span>gradientDescent_Xd()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double opt::gradientDescent_Xd </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; double(std::vector&lt; double &gt;)&gt;&#160;</td>
          <td class="paramname"><em>f</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; double &gt; &amp;&#160;</td>
          <td class="paramname"><em>x0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>tol</em> = <code>1.e-6</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compute a local minimum near initial guess of real-valued function. </p>
<p>This function implements the unconstrained Barzilai-Borwein gradient descent method (<em><a href="https://en.wikipedia.org/wiki/Gradient_descent">see Wikipedia's 'Gradient Descent' article</a></em>) for a general multi-variate real-valued function. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">f</td><td><b>(std::function&lt;double(std::vector&lt;double&gt;)&gt;)</b> real-valued function <img class="formulaInl" alt="$ f:R^d\longrightarrow R$" src="form_2.png"/> to find the local minimum nearest initial guess <img class="formulaInl" alt="$x_0$" src="form_8.png"/>. <img class="formulaInl" alt="$f$" src="form_11.png"/> is a function that has <b>std::vector&lt;double&gt;</b> arg type, and <b>double</b> as return type. </td></tr>
    <tr><td class="paramname">x0</td><td><b>(double&amp;)</b> Initial guess of gradient descent optimization problem; this parameter is updated with the optimized <img class="formulaInl" alt="$x-$" src="form_9.png"/>value prior to exiting. </td></tr>
    <tr><td class="paramname">tol</td><td><b>(double)</b> by default 1.0e-6, the convergence criterion for function gradient magnitude </td></tr>
    <tr><td class="paramname">verbose</td><td><b>(bool)</b> by default true, if set to true then the optimizer will echo to stdout number of iterations performed </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><img class="formulaInl" alt="$f(x_{opt})$" src="form_13.png"/> <b>(double)</b> local minimum of <img class="formulaInl" alt="$f$" src="form_11.png"/> in well of <img class="formulaInl" alt="$x_0$" src="form_8.png"/> </dd></dl>

</div>
</div>
<a id="a8f434753958049daab7129ea247a97ff"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8f434753958049daab7129ea247a97ff">&#9670;&nbsp;</a></span>ncgd_Xd()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">double opt::ncgd_Xd </td>
          <td>(</td>
          <td class="paramtype">std::function&lt; double(std::vector&lt; double &gt;)&gt;&#160;</td>
          <td class="paramname"><em>f</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; double &gt; &amp;&#160;</td>
          <td class="paramname"><em>x0</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double&#160;</td>
          <td class="paramname"><em>tol</em> = <code>1.e-6</code>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool&#160;</td>
          <td class="paramname"><em>verbose</em> = <code>true</code>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Nonlinear conjugate gradient descent algorithm of real-valued function. </p>
<p>This function implements an unconstrained nonlinear conjugate gradient descent method (<em><a href="https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method">see Wikipedia's article</a></em>) for a general multi-variate real-valued function. The line search performed for updating the solution guess (see the <img class="formulaInl" alt="$\alpha_n$" src="form_14.png"/> parameter in the Wikipedia article) uses the <a class="el" href="namespaceopt.html#af1ef2e32062af31429ae74fc07c57fb0" title="Compute a local minimum near initial guess of real-valued function. ">opt::gradientDescent()</a> method. <img class="formulaInl" alt="$\beta_n$" src="form_15.png"/> is computed using the Polak-Ribiere formula, also found in the Wikipedia article. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">f</td><td><b>(std::function&lt;double(std::vector&lt;double&gt;)&gt;)</b> real-valued function <img class="formulaInl" alt="$ f:R^d\longrightarrow R$" src="form_2.png"/> to find the local minimum nearest initial guess <img class="formulaInl" alt="$x_0$" src="form_8.png"/>. <img class="formulaInl" alt="$f$" src="form_11.png"/> is a function that has <b>std::vector&lt;double&gt;</b> arg type, and <b>double</b> as return type. </td></tr>
    <tr><td class="paramname">x0</td><td><b>(double&amp;)</b> Initial guess of optimization problem; this parameter is updated with the optimized <img class="formulaInl" alt="$x-$" src="form_9.png"/>value prior to exiting. </td></tr>
    <tr><td class="paramname">tol</td><td><b>(double)</b> by default 1.0e-6, the convergence criterion for function gradient magnitude </td></tr>
    <tr><td class="paramname">verbose</td><td><b>(bool)</b> by default true, if set to true then the optimizer will echo to stdout number of iterations performed </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><img class="formulaInl" alt="$f(x_{opt})$" src="form_13.png"/> <b>(double)</b> local minimum of <img class="formulaInl" alt="$f$" src="form_11.png"/> in well of <img class="formulaInl" alt="$x_0$" src="form_8.png"/> </dd></dl>
<dl class="todo"><dt><b><a class="el" href="todo.html#_todo000001">Todo:</a></b></dt><dd>Allow for user options to be set, such as which formula to use for the <img class="formulaInl" alt="$\beta_n$" src="form_15.png"/> calculations </dd></dl>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.14
</small></address>
</body>
</html>
